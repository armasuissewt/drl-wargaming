# mcts_single.py
#
# A single thread MCTS implementation.
#
# Author: Giacomo Del Rio
# Creation date: 12 Apr 2023

from __future__ import annotations

import math
import sys
from typing import List, Tuple, Dict, Optional, Generic, Callable, Literal, Any

import numpy as np

from mcts_common.mcts_policy_base import MctsPolicy
from mcts_common.mcts_utils import MinMax, ObsType
from mcts_common.rollback_env import RollbackEnv


class EpisodeStep:
    """ A step of an episode. Used also to encode a trajectory in a rollout result. """

    def __init__(self, o: Optional[ObsType], a: Optional[int], r: Optional[float], terminated: bool, truncated: bool,
                 info: Optional[Dict]):
        self.o: Optional[ObsType] = o  # Environment's observation. None for the first step
        self.a: Optional[int] = a  # The action executed right after o is observed. None for the last step
        self.r: Optional[float] = r  # The reward got from the environment after executing 'a'. None for the last step
        self.terminated: bool = terminated  # True if the episode is terminated
        self.truncated: bool = truncated  # True if the episode is truncated
        self.info: Optional[Dict] = info  # Info dictionary returned with o. None for the first step

    def __str__(self):
        return f"Step(o={str(self.o)[:5]}, a={self.a}, r={self.r}, ter={self.terminated}," \
               f" tru={self.truncated}, info={self.info})"

    def __repr__(self):
        return self.__str__()


class RolloutResult:
    """ Object to report the result of a rollout """

    def __init__(self, a: Optional[int], o: Optional[ObsType], r: Optional[float], terminated: bool,
                 truncated: bool, info: Optional[Dict], env_state: Any, obs_return: float, total_steps: int,
                 trajectory: Optional[List[EpisodeStep]], rollout_id: int = -1):
        self.a: int = a  # the action executed
        self.o: ObsType = o  # the observation from environment after executing a
        self.r: float = r  # the reward from environment for executing a
        self.terminated: bool = terminated  # if environment tells terminated
        self.truncated: bool = truncated  # if environment tells truncated
        self.info = info  # the info from environment after executing a
        self.env_state = env_state  # the internal environment's state after executing a
        self.obs_return: float = obs_return  # The computed return for o (contains estimate on truncated rollouts)
        self.total_steps: int = total_steps  # number of time step() is called in the env (>= 1)
        self.rollout_id: int = rollout_id  # unique rollout ID (parallel algos only)

        # If present, the trajectory of the rollout excluding the part generated by leading_actions
        self.trajectory: Optional[List[EpisodeStep]] = trajectory
        assert total_steps >= 1


class MCTSSingle(Generic[ObsType]):
    """ Monte Carlo tree search

        This class holds a tree of MctsNode objects. Each node has 1 parent and either 0 or n_actions children.
        Only environments with a discrete actions are supported. (i.e. gym.spaces.Discrete).
        Actions are always identified with a single, 0-based, integer.
        Environment observations have generic type ObsType.
        Probability vectors are supposed to be numpy arrays: using tf.Tensor or torch.Tensor for probability
        vectors may not work as expected.
    """

    def __init__(self, env: RollbackEnv, policy: MctsPolicy[ObsType], root_obs: ObsType, root_info: Dict,
                 n_actions: int, max_rollout_steps: int, gamma: float, uct_policy_c: float, uct_exploration_c: float,
                 add_exploration_noise: bool, exploration_noise_dirichlet_alpha: float,
                 exploration_noise_fraction: bool, random_rollout: bool, rollout_start_node: Literal['root', 'leaf'],
                 min_max: MinMax, rnd_seed: int = None, callbacks: Dict[str, Callable] = None):
        """ Initialize the MCTS tree

        :param env: the environment, whose internal state is aligned with root_obs and root_info
        :param policy: policy and value network
        :param root_obs: the observation for the root node of the tree
        :param root_info: info dictionary for the root node of the tree
        :param n_actions: number of available actions at each step
        :param max_rollout_steps: number of steps before truncating the rollout (can be 0)
        :param gamma: discount parameter
        :param uct_policy_c: Upper Confidence score for Tree (UCT) policy intensity parameter [0, 1]
        :param uct_exploration_c: Upper Confidence score for Tree (UCT) weighting parameter for exploration >= 0
        :param add_exploration_noise: if True, adds some noise to the policy of the tree node to favor exploration
        :param exploration_noise_dirichlet_alpha: the alpha parameter of the Dirichlet noise to add
        :param exploration_noise_fraction: how much the noise will impact the policy [0, 1]
        :param random_rollout: if True, performs purely random rollouts, otherwise use policy
        :param rollout_start_node: if 'root' restart the rollouts always from the root node, if 'leaf' start from the
            deepest node available. With 'root', only one env state is saved per each build() call, with 'leaf', each
            tree node will contain the saved state of the env, with much higher memory consumption.
        :param min_max: use this MinMax to normalize Q values (same MinMax can be reused across episodes)
        :param rnd_seed: the seed of the random number generator
        :param callbacks: callbacks dictionary. Available callbacks:
            'on_rollout': called at the end of each rollout
                signature: (trajectory: List[EpisodeStep], last_obs_value: float, from_nn: bool)
            'on_debug': called in various point of the code to allow debugging
                signature: (mcts: MCTS, place: str)
        """
        self.env: RollbackEnv = env
        self.policy: MctsPolicy[ObsType] = policy
        self.n_actions = n_actions
        self.max_rollout_steps = max_rollout_steps
        self.gamma = gamma
        self.uct_policy_c = uct_policy_c
        self.uct_exploration_c = uct_exploration_c
        self.add_expl_noise = add_exploration_noise
        self.expl_dirichlet_alpha = exploration_noise_dirichlet_alpha
        self.expl_fraction = exploration_noise_fraction
        self.random_rollout = random_rollout
        self.rollout_start_node = rollout_start_node
        self.min_max = min_max
        self.rnd_gen = np.random.default_rng(rnd_seed)
        self.callbacks = callbacks if callbacks else {}
        MCTSSingle.validate_callbacks(self.callbacks)
        self.total_rollouts: int = 0
        self.total_rollouts_steps: int = 0

        if rollout_start_node not in ['root', 'leaf']:
            raise RuntimeError(f"Unknown value '{rollout_start_node}' for parameter rollout_start_node")

        self.root: MctsNode[ObsType] = self.make_root_node(root_obs, root_info, env.get_env_state())

    def make_root_node(self, root_obs: ObsType, root_info: Dict, env_state: Any) -> MctsNode[ObsType]:
        """ Create and return the root node of the tree

        :param root_obs: the observation of the root node
        :param root_info: the info dictionary associated with root_obs
        :param env_state: the internal state of the environment aligned with root_obs and root_info
        :return: the newly created root node
        """
        root_n = MctsNode[ObsType](parent=None, a=None, env_state=env_state)
        root_n.obs = root_obs
        root_n.info = root_info
        pi = self.policy.predict_p(root_n.obs)
        if self.add_expl_noise:
            expl_noise = self.rnd_gen.dirichlet([self.expl_dirichlet_alpha] * self.n_actions)
            pi = self.add_exploration_noise(pi, expl_noise, self.expl_fraction)
        root_n.expand(self.n_actions, pi)
        return root_n

    def build(self, n_nodes: int) -> None:
        """ Add n_nodes nodes to the tree.
            Assume that root state and environment are aligned.
            The environment will be left unchanged at the end of this function.

        :param n_nodes: how many nodes to add to the tree (number of tree expansions)
        """
        if n_nodes == 0:
            return

        for _ in range(n_nodes):
            # --- Selection
            selected_node, actions = self.find_non_visited_leaf_node(self.root)
            if 'on_debug' in self.callbacks:
                self.callbacks['on_debug'](self, "build-select", info={'sel_node': selected_node})

            if selected_node.is_final():
                # --- Fast value update for already final nodes
                selected_node.update_return(0 if selected_node.terminated else selected_node.truncated_val)
            else:
                # --- Rollout
                if self.rollout_start_node == 'leaf':
                    rollout_res = self.safe_rollout(selected_node.parent, actions[-1], [])
                else:  # root
                    rollout_res = self.safe_rollout(self.root, actions[-1], actions[:-1])
                self.total_rollouts_steps += rollout_res.total_steps
                self.total_rollouts += 1

                # --- Value update for node
                self.update_node(selected_node, rollout_res)

            # --- Tree update
            self.backpropagate(selected_node)
            if 'on_debug' in self.callbacks:
                self.callbacks['on_debug'](self, "build-update", info={})

        self.env.set_env_state(self.root.env_state, rebuild_env=False)

    def find_non_visited_leaf_node(self, current_node: MctsNode[ObsType]) -> Tuple[MctsNode[ObsType], List[int]]:
        """ Starting from current_node, find a node in the tree that is non-visited and leaf.
            **NB: If during the search a final node is encountered, it is returned.**

        :param current_node: node to start the search from
        :return: the selected non-visited leaf node and the action list to reach that node from current_node
        """
        actions = []
        while (current_node.is_visited() or not current_node.is_leaf()) and not current_node.is_final():
            if current_node.is_visited() and current_node.is_leaf():
                pi = self.policy.predict_p(current_node.obs) if current_node.obs is not None else None
                current_node.expand(self.n_actions, pi)

            a = current_node.select_best_child(self.uct_policy_c, self.uct_exploration_c, self.gamma, self.min_max)
            actions.append(a)
            current_node = current_node.child_for_action(a)

        return current_node, actions

    def update_node(self, nv_leaf_node: MctsNode, rollout_res: RolloutResult) -> None:
        """ Update the statistics of a non-visited, leaf node, after a rollout

        :param nv_leaf_node: the non-visited, leaf node to be updated
        :param rollout_res: a RolloutResult object
        """
        assert nv_leaf_node.a == rollout_res.a
        nv_leaf_node.r = rollout_res.r
        nv_leaf_node.obs = rollout_res.o
        nv_leaf_node.info = rollout_res.info
        nv_leaf_node.terminated = rollout_res.terminated
        nv_leaf_node.env_state = rollout_res.env_state
        nv_leaf_node.update_return(rollout_res.obs_return)
        if rollout_res.truncated:
            nv_leaf_node.truncated_val = rollout_res.obs_return

        assert not (nv_leaf_node.terminated and (nv_leaf_node.truncated_val is not None))

    def backpropagate(self, initial_node: MctsNode[ObsType]) -> None:
        """ Update backward the statistics of the tree starting from initial_node

        :param initial_node: the node to start the backpropagation (it is never updated, only the ancestors are)
        """
        last_action = initial_node.a
        current_node = initial_node.parent
        while current_node is not None:
            q = current_node.children[last_action].r + self.gamma * current_node.children[last_action].V
            current_node.update_return(q)
            self.min_max.update(q)

            last_action = current_node.a
            current_node = current_node.parent

    def safe_rollout(self, start_node: MctsNode, action: int, leading_actions: List[int]) -> RolloutResult:
        """ Execute self.rollout() and return the result.
            In case of exceptions, restore the checkpoint and retry up to self.env.max_retry_on_error times.
        :return: same as rollout()
        """
        attempt = 0
        last_error = None
        while attempt < self.env.max_retry_on_error:
            try:
                rollout_res = self.rollout(start_node.env_state, action, leading_actions)
                if 'on_rollout' in self.callbacks:
                    if len(leading_actions) == 0:
                        rollout_res.trajectory[0].o = start_node.obs
                        rollout_res.trajectory[0].info = start_node.info
                    self.callbacks['on_rollout'](rollout_res.trajectory)
                return rollout_res
            except Exception as e:
                print(f"safe_rollout: step() error {attempt + 1}/{self.env.max_retry_on_error}. {e}", file=sys.stderr)
                last_error = e
                attempt += 1
                self.env.rebuild_env()
        raise RuntimeError(f"the safe_rollout() failed with {last_error}")

    def rollout(self, env_state: Any, action: int, leading_actions: List[int]) -> RolloutResult:
        """ Set the environment to env_state, then execute 'action', then execute a maximum of max_rollout_steps steps.
        If leading_actions is given, executes such actions before 'action': this is needed when starting from the root
        state to reach some deep state in the tree.
        When max_rollout_steps is reached or the environment returns 'truncated', it truncates the rollout by
        asking the Neural Network an estimated value for the last observation visited.
        At the beginning of the rollout, initial_actions list is used to guide the rollout.
        For the rollout part, the actions are selected with a probability proportional to the output of the policy
        neural network if the random_rollout parameter is False, otherwise are selected randomly.

        :param env_state: the environment state to be set at the beginning
        :param action: the action to take
        :param leading_actions: optional leading actions before 'action' is executed
        :return: a RolloutResult
        """
        self.env.set_env_state(env_state, rebuild_env=False)

        actions = leading_actions + [action]
        trajectory: List[EpisodeStep] = []
        new_env_state: Optional[Any] = None
        current_obs, current_info = None, None
        terminated, truncated, max_steps_reached = False, False, False
        step = 0
        while not (terminated or truncated or max_steps_reached):
            if step < len(actions):
                action = actions[step]
            else:
                if self.random_rollout:
                    action = self.rnd_gen.choice(self.n_actions)
                else:
                    pi = self.policy.predict_p(current_obs)  # Never called on first step
                    action = self.rnd_gen.choice(self.n_actions, p=pi)

            prev_obs = current_obs
            prev_info = current_info
            current_obs, reward, terminated, truncated, current_info = self.env.step(action)
            step += 1
            if step >= len(actions):
                trajectory.append(EpisodeStep(prev_obs, action, reward, False, False, prev_info))
            if step == len(actions):
                new_env_state = self.env.get_env_state()
            if step >= (self.max_rollout_steps + len(actions)):
                max_steps_reached = True

        trajectory.append(EpisodeStep(current_obs, None, None, terminated, truncated, current_info))
        last_obs_value = self.policy.predict_v(current_obs) if not terminated else 0
        obs_return = self.compute_return(trajectory[1:], last_obs_value, self.gamma)
        return RolloutResult(actions[-1], trajectory[1].o, trajectory[0].r, trajectory[1].terminated,
                             trajectory[1].truncated, trajectory[1].info, new_env_state, obs_return, total_steps=step,
                             trajectory=trajectory)

    def advance_root(self, action: int, env: RollbackEnv, new_root_obs: np.ndarray, new_root_info: Dict) -> None:
        """ Move the root forward by one step according to action

        :param action: action to select the child
        :param env: the environment, whose internal state is aligned with next_obs and next_info
        :param new_root_obs: environment observation of the new root
        :param new_root_info: environment info dictionary for the new root
        """
        self.env = env
        self.root = self.root.children[action]
        self.root.parent = None
        self.root.obs = new_root_obs
        self.root.info = new_root_info
        self.root.env_state = env.get_env_state()

        if self.root.is_leaf():
            pi = self.policy.predict_p(self.root.obs)
            self.root.expand(self.n_actions, pi)

        if self.add_expl_noise:
            expl_noise = self.rnd_gen.dirichlet([self.expl_dirichlet_alpha] * self.n_actions)
            self.root.P = self.add_exploration_noise(self.root.P, expl_noise, self.expl_fraction)

    def root_action_preferences(self, criterion: Literal['counts', 'qvalues', 'policy']) -> np.ndarray:
        """ Get the action preferences for the root node estimated using the given criterion.
        If criterion == 'counts', the action preferences are proportional to the visits count of each child.
        If criterion == 'qvalues', the action preferences are proportional to the estimated Q-value of each child.
        If criterion == 'policy', the action preferences are taken from the output of the policy network only.
            Use only in conjunction with num_expansions=0. It can be used to evaluate the agent behavior uniquely
            on the trained policy.

        :param criterion: one among 'qvalues', 'counts', 'policy'
        :return: the preferences array
        """
        if criterion == 'qvalues':
            action_preferences = [child.r + self.gamma * child.V if child.N > 0 else self.min_max.min
                                  for child in self.root.children]
            return self.min_max.normalize(np.array(action_preferences))
        elif criterion == 'counts':
            return np.array([child.N for child in self.root.children])
        elif criterion == 'policy':
            return self.root.P
        else:
            raise NotImplementedError(f"Criterion {criterion} not implemented")

    @staticmethod
    def add_exploration_noise(probs: np.ndarray, noise: np.ndarray,
                              exploration_fraction: Optional[float]) -> np.ndarray:
        """ Add noise to the action probabilities in probs

        :param probs: action probabilities vector
        :param noise: noise vector (positive and sum to 1)
        :param exploration_fraction: optional weight of noise in [0, 1]. If present, the output probabilities are
            a linear combination of probs and noise: the close to 1, the more noise.
            If None, noise is simply added to probs and the output is no more a probability array (sum > 1)
        :return: the perturbed action probabilities array
        """
        if exploration_fraction is not None:
            probs = probs * (1 - exploration_fraction) + noise * exploration_fraction
            return probs / np.sum(probs)
        else:
            return probs + noise

    @staticmethod
    def validate_callbacks(callbacks: Dict) -> None:
        """ Validate the callbacks' dictionary. If some keys are unknown a ValueError is raised

        :param callbacks: callbacks dictionary
        """
        available_keys = ['on_rollout', 'on_debug']
        for k in callbacks:
            if k not in available_keys:
                raise ValueError(f"Unknown parameter '{k}' in callbacks")

    @staticmethod
    def compute_return(trajectory: List[EpisodeStep], last_obs_value: float, gamma: float) -> float:
        """ Compute the return of the first observation of the trajectory

        :param trajectory: an episode trajectory
        :param last_obs_value: value of last observation (should be 0 if final observation)
        :param gamma: discount factor
        :return: the discounted return of the first observation of the trajectory
        """
        res = last_obs_value
        for step in reversed(trajectory[:-1]):
            res = step.r + gamma * res
        return res


class MctsNode(Generic[ObsType]):
    """ Node of the MCTS tree.

        Environment observations have generic type ObsType.
    """

    def __init__(self, parent: Optional[MctsNode[ObsType]], a: Optional[int], env_state: Optional[Any] = None):
        self.parent: Optional[MctsNode[ObsType]] = parent  # parent node (None for root)
        self.children: List[MctsNode[ObsType]] = []  # child nodes (empty for leaves)
        self.a: Optional[int] = a  # action that brings to this node from parent (None for root)
        self.r: Optional[float] = None  # reward received from environment by executing action a from parent
        self.obs: Optional[ObsType] = None  # observation as returned by the environment
        self.info: Optional[Dict] = None  # additional information from environment
        self.P: Optional[np.ndarray] = None  # policy from Neural Network (same len as children)
        self.env_state: Optional[Any] = env_state  # saved internal environment state
        self.terminated: bool = False  # If true, the environment returned terminated with obs
        # If not None, the environment returned truncated with obs and the value is the NN estimate for obs
        self.truncated_val: Optional[float] = None

        self.virtual: int = 0  # number of virtual-visits of the node (forward)
        self.N: int = 0  # number of visits of the node (backward)
        self.W: float = 0  # sum of returns of each visit
        self.V: float = 0  # V = W / N (sampled node value)

    def __repr__(self):
        return f"MctsNode{self.path_from_root()}"

    def __str__(self):
        return self.__repr__()

    def is_root(self) -> bool:
        """ Check if node is the root
        :return: True if is root, False otherwise
        """
        return self.parent is None

    def is_visited(self, virtual: bool = False) -> bool:
        """ Check if this node has been visited
        :param virtual: if True, takes into account virtual-visits
        :return: True if the node has been (virtual)visited at least once, False otherwise
        """
        return (self.N + self.virtual) > 0 if virtual else self.N > 0

    def is_leaf(self) -> bool:
        """ Check if this node is a leaf (i.e. it has children)
        :return: True if is a leaf, False otherwise
        """
        return len(self.children) == 0

    def is_final(self) -> bool:
        """ Check if this node holds a terminated or truncated observation

        :return: True if it is final, False otherwise
        """
        return self.terminated | (self.truncated_val is not None)

    def path_from_root(self) -> List[int]:
        """ Compute the path that goes from the root to this node

        :return: the list of action to go from the root to this node. Empty for root
        """
        curr_node, path = self, []
        while curr_node is not None:
            path.insert(0, curr_node.a)
            curr_node = curr_node.parent
        return path[1:]

    def update_return(self, v: float) -> None:
        """ Update the expected return of the node (used in backward pass)

        :param v: the sampled return
        """
        self.N += 1
        self.W += v
        self.V = self.W / self.N

    def select_best_child(self, policy_c: float, exploration_c: float, gamma: float, min_max: MinMax) -> int:
        """ Select the best action using Upper Confidence scores for Tree (UCT) of children

        :param policy_c: UCT weighting parameter for policy
        :param exploration_c: UCT weighting parameter for exploration
        :param gamma: discount factor
        :param min_max: a MinMax objects with minimum and maximum node values seen so far
        :return: action that brings to the child with the highest UCT
        """
        # Compute UCT for each child
        ucts = []
        pi = self.P if self.P is not None else np.ones(len(self.children)) / len(self.children)
        for child, p in zip(self.children, pi):
            ucts.append(child.uct_score(p, policy_c, exploration_c, gamma, min_max))

        return int(np.argmax(ucts))  # Unfair tie breaking

    def uct_score(self, prior: float, policy_c: float, exploration_c: float, gamma: float, min_max: MinMax) -> float:
        """ Compute the Upper Confidence scores for Tree (UCT) (of the action) of this node

        :param prior: the probability to go to child from parent as returned by the policy Neural Network
        :param policy_c: UCT policy intensity in [0, 1]. 0: no policy impact. 1: full policy impact (AlphaZero)
        :param exploration_c: UCT exploration parameter. The bigger => more exploration
        :param gamma: discount factor
        :param min_max: a MinMax objects with minimum and maximum node values seen so far
        :return: the UCT score of this node
        """
        # uct_prior = c * math.sqrt(math.log(self.parent.N) / (self.N if self.N > 0 else 1))  # Original UCT
        # uct_prior = c * prior * math.sqrt(self.parent.N / (self.N if self.N > 0 else 1))  # AlphaZero hi growth
        policy_intensity = (1 - policy_c) * (1 / len(self.parent.children)) + policy_c * prior
        uct_prior = exploration_c * policy_intensity
        uct_prior *= math.sqrt(self.parent.N + self.parent.virtual) / (self.N + self.virtual + 1)  # AlphaZero

        if self.N > 0:
            uct_q = self.r + gamma * self.V
            uct_q = min_max.normalize(uct_q)
        else:
            uct_q = 0

        return uct_q + uct_prior

    def child_for_action(self, a: int) -> MctsNode[ObsType]:
        """ Return the node reached by executing a from this node

        :param a: the action
        :return: the node reached by executing a from this node
        """
        return self.children[a]

    def expand(self, n_actions: int, policy: np.ndarray) -> None:
        """ Expand the current node by creating one child for each action

        :param n_actions: number of feasible actions
        :param policy: the action probabilities as returned by the policy
        """
        for a in range(n_actions):
            self.children.append(MctsNode[ObsType](parent=self, a=a))
        self.P = policy
